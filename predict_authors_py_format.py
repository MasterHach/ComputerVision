# -*- coding: utf-8 -*-
"""
–í–µ—Ä—Å–∏—è –∫–æ–¥–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ .py
"""

# ========== 1 –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ =================
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import os
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
from tqdm import tqdm

import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# ========== 2 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö =================

chunks = []
authors = []
device = torch.device('cuda')
model_name = "cointegrated/rubert-tiny"  # –õ–µ–≥–∫–∞—è –≤–µ—Ä—Å–∏—è ruBERT
tokenizer = AutoTokenizer.from_pretrained(model_name)

max_tokens = 512
chunk_size = 5 # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –≤—Ö–æ–¥–∏—Ç—å –≤ 1 –∑–∞–ø–∏—Å—å –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
author_dir = 'authors' # –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
train_dir = 'train' # –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
stop_words = set(stopwords.words('russian'))

# ========== 3 –°–±–æ—Ä –∏ –ø—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö =================

# –ø—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º 6 —Ñ–∞–π–ª–∞–º, –ø—Ä–µ–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç, —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
for file in os.listdir(author_dir):
    """
    –ù–µ –±—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, —É–¥–∞–ª–µ–Ω–∏—è —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤, —É–¥–∞–ª–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤, –ª–µ–º–º–∏–Ω–≥–∞ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞, —Ç–∞–∫ –∫–∞–∫
    –ø–æ–∫–∞–∑–∞–ª–æ—Å—å, —á—Ç–æ —É–¥–∞–ª–∏–≤ –º–æ–∂–µ—Ç –ø—Ä–æ–ø–∞—Å—Ç—å –∞–≤—Ç–æ—Ä—Å–∫–∏–π —Å—Ç–∏–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä –º–Ω–æ–≥–æ —Ç–∏—Ä–µ –≤ —Ç–µ–∫—Å—Ç–µ - –∑–Ω–∞—á–∏—Ç –¥–∏–∞–ª–æ–≥, –∞–≤—Ç–æ—Ä—Å–∫–∏–π –ø—Ä–∏–µ–º,
    –º–Ω–æ–≥–æ ! –≤ —Ç–µ–∫—Å—Ç–µ - —Ç–æ–∂–µ –Ω–µ —É –∫–∞–∂–¥–æ–≥–æ –∞–≤—Ç–æ—Ä–∞)
    """
    author = file.split('.')[0]
    path = os.path.join(author_dir, file)
    with open(path, encoding='utf-8') as f:
        text = f.read()
        text = text.lower() # –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –Ω–∞–ø—Ä–∏–º–µ—Ä –¢–µ–∫—Å—Ç, —Ç–µ–∫—Å—Ç
        text = " ".join([word for word in text.split() if word not in stop_words]) # —É–¥–∞–ª–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∏ –≤ –Ω–∞ –∫ –æ—Ç (–Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ)
        sentences = sent_tokenize(text, language="russian")
        for i in range(0, len(sentences), chunk_size):
            chunk = " ".join(sentences[i:i+chunk_size])
            if len(tokenizer(chunk)["input_ids"]) <= max_tokens:
                chunks.append(chunk)
                authors.append(author)

df = pd.DataFrame({
    'chunk': chunks,
    'author': authors
})

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6).to("cuda")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
def tokenize_function(text):
    return tokenizer(text, padding='max_length', truncation=True, max_length=512)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º
df['input_ids'] = df['chunk'].apply(lambda x: tokenize_function(x)['input_ids'])
df['attention_mask'] = df['chunk'].apply(lambda x: tokenize_function(x)['attention_mask'])

author_to_id = {'Bradbury': 0, 'Bulgakov': 1, 'Fry': 2, 'Genri': 3, 'Simak': 4, 'Strugatskie': 5}
df['labels'] = df['author'].map(author_to_id)

# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
train_texts, val_texts, train_labels, val_labels = train_test_split(df['chunk'], df['labels'], test_size=0.2, shuffle=True)

# –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ä–∞–∑–±–∏—Ç—ã—Ö –Ω–∞ —á–∞–Ω–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–≤ —Ç.—á —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
class AuthorDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, item):
        text = self.texts[item]
        label = self.labels[item]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
train_dataset = AuthorDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)
val_dataset = AuthorDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)


# ================ 4 –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ================


# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='./results',          # –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –º–æ–¥–µ–ª—å
    num_train_epochs=3,              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    per_device_train_batch_size=8,   # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    per_device_eval_batch_size=8,    # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    warmup_steps=500,                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞
    weight_decay=0.01,               # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    logging_dir='./logs',            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logging_steps=10,
    eval_strategy="epoch",           # –û—Ü–µ–Ω–∫–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
    fp16=True
)

# –°–æ–∑–¥–∞–µ–º Trainer
trainer = Trainer(
    model=model,                         # –ú–æ–¥–µ–ª—å
    args=training_args,                  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    train_dataset=train_dataset,         # –û–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
    eval_dataset=val_dataset             # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
)

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
trainer.train()


# ================ 5 –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö ====================

predictions = []

# –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–∞–ø–∫–∏ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ —Å—Ä–∞–∑—É –∂–µ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

for file in os.listdir(train_dir):
  with open(os.path.join(train_dir, file), "r", encoding="utf-8") as f:
        text = f.read()

  inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to("cuda")
  with torch.no_grad():
      outputs = model(**inputs)
      probs = torch.nn.functional.softmax(outputs.logits, dim=1)
      predicted_class = torch.argmax(probs, dim=1).item()

  predicted_author = [k for k, v in author_to_id.items() if v == predicted_class]
  predictions.append((file, "".join(predicted_author).lower()))

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∏ –≤—ã–≥—Ä—É–∑–∫–∞ –≤ csv

df = pd.DataFrame(predictions, columns=["filename", "author"])
df.to_csv("predictions.csv", index=False)

# ================ 6 –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ ======================

pred_df = pd.read_csv("predictions.csv")
true_df = pd.read_csv("corrects.csv") # —Ñ–∞–π–ª –≥–¥–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –≤–µ—Ä–Ω—ã–µ –∞–≤—Ç–æ—Ä—ã –æ—Ç—Ä—ã–≤–∫–æ–≤ 

# –û–±—ä–µ–¥–∏–Ω–∏–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
merged = pd.merge(pred_df, true_df, on='filename')
merged.columns = ['filename', 'predicted_author', 'true_author', 'class']

y_true = merged['true_author']
y_pred = merged['predicted_author']

# –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏

print("üîé Accuracy:", accuracy_score(y_true, y_pred))
print("üîé F1-score (macro):", f1_score(y_true, y_pred, average='macro'))

print("\nüìä Classification Report:")
print(classification_report(y_true, y_pred))

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫

labels = sorted(y_true.unique())

cm = confusion_matrix(y_true, y_pred, labels=labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫")
plt.tight_layout()
plt.show()

# –ì—Ä–∞—Ñ–∏–∫ F1-score –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É (–∞–≤—Ç–æ—Ä—É)

report = classification_report(y_true, y_pred, output_dict=True)
f1_scores = {label: report[label]['f1-score'] for label in labels}

plt.figure(figsize=(10, 6))
sns.barplot(x=list(f1_scores.keys()), y=list(f1_scores.values()), palette='viridis')
plt.ylabel('F1-score')
plt.title('F1-score –ø–æ –∞–≤—Ç–æ—Ä–∞–º')
plt.ylim(0, 1.05)
plt.tight_layout()
plt.show()