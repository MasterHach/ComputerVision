# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xi0Rn2dobN9BBDfsc3rF8ol6g15CK9u5
"""

from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import os
import pandas as pd
import torch
from transformers import AutoTokenizer

import nltk
chunks = []
authors = []
device = torch.device('cuda')
nltk.download('punkt_tab')
model_name = "cointegrated/rubert-tiny"  # –õ–µ–≥–∫–∞—è –≤–µ—Ä—Å–∏—è ruBERT
tokenizer = AutoTokenizer.from_pretrained(model_name)

max_tokens = 512
chunk_size = 5 # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –≤—Ö–æ–¥–∏—Ç—å –≤ 1 –∑–∞–ø–∏—Å—å –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
author_dir = 'authors'
train_dir = 'train'
stop_words = set(stopwords.words('russian'))

# –ø—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º 6 —Ñ–∞–π–ª–∞–º, –ø—Ä–µ–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç, —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
for file in os.listdir(author_dir):
    """
    –ù–µ –±—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, —É–¥–∞–ª–µ–Ω–∏—è —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤, —É–¥–∞–ª–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤, –ª–µ–º–º–∏–Ω–≥–∞ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞, —Ç–∞–∫ –∫–∞–∫
    –ø–æ–∫–∞–∑–∞–ª–æ—Å—å, —á—Ç–æ —É–¥–∞–ª–∏–≤ –º–æ–∂–µ—Ç –ø—Ä–æ–ø–∞—Å—Ç—å –∞–≤—Ç–æ—Ä—Å–∫–∏–π —Å—Ç–∏–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä –º–Ω–æ–≥–æ —Ç–∏—Ä–µ –≤ —Ç–µ–∫—Å—Ç–µ - –∑–Ω–∞—á–∏—Ç –¥–∏–∞–ª–æ–≥, –∞–≤—Ç–æ—Ä—Å–∫–∏–π –ø—Ä–∏–µ–º,
    –º–Ω–æ–≥–æ ! –≤ —Ç–µ–∫—Å—Ç–µ - —Ç–æ–∂–µ –Ω–µ —É –∫–∞–∂–¥–æ–≥–æ –∞–≤—Ç–æ—Ä–∞)
    """
    author = file.split('.')[0]
    path = os.path.join(author_dir, file)
    with open(path, encoding='utf-8') as f:
        text = f.read()
        text = text.lower() # –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –Ω–∞–ø—Ä–∏–º–µ—Ä –¢–µ–∫—Å—Ç, —Ç–µ–∫—Å—Ç
        text = " ".join([word for word in text.split() if word not in stop_words]) # —É–¥–∞–ª–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∏ –≤ –Ω–∞ –∫ –æ—Ç (–Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ)
        sentences = sent_tokenize(text, language="russian")
        for i in range(0, len(sentences), chunk_size):
            chunk = " ".join(sentences[i:i+chunk_size])
            if len(tokenizer(chunk)["input_ids"]) <= max_tokens:
                chunks.append(chunk)
                authors.append(author)

df = pd.DataFrame({
    'chunk': chunks,
    'author': authors
})

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6).to("cuda")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
def tokenize_function(text):
    return tokenizer(text, padding='max_length', truncation=True, max_length=512)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º
df['input_ids'] = df['chunk'].apply(lambda x: tokenize_function(x)['input_ids'])
df['attention_mask'] = df['chunk'].apply(lambda x: tokenize_function(x)['attention_mask'])

author_to_id = {'Bradbury': 0, 'Bulgakov': 1, 'Fry': 2, 'Genri': 3, 'Simak': 4, 'Strugatskie': 5}
df['labels'] = df['author'].map(author_to_id)

from sklearn.model_selection import train_test_split

# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
train_texts, val_texts, train_labels, val_labels = train_test_split(df['chunk'], df['labels'], test_size=0.2, shuffle=True)

import torch
from torch.utils.data import Dataset

class AuthorDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, item):
        text = self.texts[item]
        label = self.labels[item]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
train_dataset = AuthorDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)
val_dataset = AuthorDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)

from transformers import Trainer, TrainingArguments

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='./results',          # –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –º–æ–¥–µ–ª—å
    num_train_epochs=3,              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    per_device_train_batch_size=8,   # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    per_device_eval_batch_size=8,    # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    warmup_steps=500,                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞
    weight_decay=0.01,               # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    logging_dir='./logs',            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logging_steps=10,
    eval_strategy="epoch",           # –û—Ü–µ–Ω–∫–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
    fp16=True
)

# –°–æ–∑–¥–∞–µ–º Trainer
trainer = Trainer(
    model=model,                         # –ú–æ–¥–µ–ª—å
    args=training_args,                  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    train_dataset=train_dataset,         # –û–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
    eval_dataset=val_dataset             # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
)

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
trainer.train()

from tqdm import tqdm

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö

predictions = []

for file in os.listdir(train_dir):
  with open(os.path.join(train_dir, file), "r", encoding="utf-8") as f:
        text = f.read()

  inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to("cuda")
  with torch.no_grad():
      outputs = model(**inputs)
      probs = torch.nn.functional.softmax(outputs.logits, dim=1)
      predicted_class = torch.argmax(probs, dim=1).item()

  predicted_author = [k for k, v in author_to_id.items() if v == predicted_class]
  predictions.append((file, "".join(predicted_author).lower()))

df = pd.DataFrame(predictions, columns=["filename", "author"])
df.to_csv("predictions.csv", index=False)

import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===

pred_df = pd.read_csv("predictions.csv")
true_df = pd.read_csv("corrects.csv") # —Ñ–∞–π–ª –≥–¥–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –≤–µ—Ä–Ω—ã–µ –∞–≤—Ç–æ—Ä—ã

# –û–±—ä–µ–¥–∏–Ω–∏–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
merged = pd.merge(pred_df, true_df, on='filename')
merged.columns = ['filename', 'predicted_author', 'true_author', 'class']

y_true = merged['true_author']
y_pred = merged['predicted_author']

# === 2. –ú–µ—Ç—Ä–∏–∫–∏ ===

print("üîé Accuracy:", accuracy_score(y_true, y_pred))
print("üîé F1-score (macro):", f1_score(y_true, y_pred, average='macro'))

print("\nüìä Classification Report:")
print(classification_report(y_true, y_pred))

# === 3. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ ===

labels = sorted(y_true.unique())

cm = confusion_matrix(y_true, y_pred, labels=labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫")
plt.tight_layout()
plt.show()

# === 4. –ë–∞—Ä-—á–∞—Ä—Ç—ã –¥–ª—è F1-score –ø–æ –∫–∞–∂–¥–æ–º—É  –∫–ª–∞—Å—Å—É ===

report = classification_report(y_true, y_pred, output_dict=True)
f1_scores = {label: report[label]['f1-score'] for label in labels}

plt.figure(figsize=(10, 6))
sns.barplot(x=list(f1_scores.keys()), y=list(f1_scores.values()), palette='viridis')
plt.ylabel('F1-score')
plt.title('F1-score –ø–æ –∞–≤—Ç–æ—Ä–∞–º')
plt.ylim(0, 1.05)
plt.tight_layout()
plt.show()